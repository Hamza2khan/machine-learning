{
    "cells": [
        {
            "cell_type": "code",
            "metadata": {},
            "source": "!pip install -q /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n!pip install -q /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n!pip install -q /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n!pip install -q /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n!pip install -q /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl\n!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/pytorch_lightning-2.4.0-py3-none-any.whl\n!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/torchmetrics-1.5.2-py3-none-any.whl\n!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/pytorch_tabnet-4.1.0-py3-none-any.whl\n!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/einops-0.7.0-py3-none-any.whl\n!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/pytorch_tabular-1.1.1-py2.py3-none-any.whl"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "from pathlib import Path\nfrom metric import score\nimport pandas as pd\nimport numpy as np\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nROOT_DATA_PATH = Path(r\"/kaggle/input/equity-post-HCT-survival-predictions\")\n\npd.set_option('display.max_columns', 100)\n\ntrain = pd.read_csv(ROOT_DATA_PATH.joinpath(\"train.csv\"))\ntest = pd.read_csv(ROOT_DATA_PATH.joinpath(\"test.csv\"))\n\nCATEGORICAL_VARIABLES = [\n    # Graft and HCT reasons\n    'dri_score', 'graft_type', 'prod_type', 'prim_disease_hct',\n\n    # Patient health status (risk factors)\n    'psych_disturb', 'diabetes', 'arrhythmia', 'vent_hist', 'renal_issue', 'pulm_moderate',\n    'pulm_severe', 'obesity', 'hepatic_mild', 'hepatic_severe', 'peptic_ulcer', 'rheum_issue',\n    'cardiac', 'prior_tumor', 'mrd_hct', 'tbi_status', 'cyto_score', 'cyto_score_detail', \n\n    # Patient demographics\n    'ethnicity', 'race_group',\n\n    # Biological matching with donor\n    'sex_match', 'donor_related', 'cmv_status', 'tce_imm_match', 'tce_match', 'tce_div_match',\n\n    # Medication/operation related data\n    'melphalan_dose', 'rituximab', 'gvhd_proph', 'in_vivo_tcd', 'conditioning_intensity'\n]\n\nHLA_COLUMNS = [\n    'hla_match_a_low', 'hla_match_a_high',\n    'hla_match_b_low', 'hla_match_b_high',\n    'hla_match_c_low', 'hla_match_c_high',\n    'hla_match_dqb1_low', 'hla_match_dqb1_high',\n    'hla_match_drb1_low', 'hla_match_drb1_high',\n    \n    # Matching at HLA-A(low), -B(low), -DRB1(high)\n    'hla_nmdp_6',\n    # Matching at HLA-A,-B,-DRB1 (low or high)\n    'hla_low_res_6', 'hla_high_res_6',\n    # Matching at HLA-A, -B, -C, -DRB1 (low or high)\n    'hla_low_res_8', 'hla_high_res_8',\n    # Matching at HLA-A, -B, -C, -DRB1, -DQB1 (low or high)\n    'hla_low_res_10', 'hla_high_res_10'\n]\n\nOTHER_NUMERICAL_VARIABLES = ['year_hct', 'donor_age', 'age_at_hct', 'comorbidity_score', 'karnofsky_score']\nNUMERICAL_VARIABLES = HLA_COLUMNS + OTHER_NUMERICAL_VARIABLES\n\nTARGET_VARIABLES = ['efs_time', 'efs']\nID_COLUMN = [\"ID\"]\n\ndef preprocess_data(df):\n    df[CATEGORICAL_VARIABLES] = df[CATEGORICAL_VARIABLES].fillna(\"Unknown\")\n    df[OTHER_NUMERICAL_VARIABLES] = df[OTHER_NUMERICAL_VARIABLES].fillna(df[OTHER_NUMERICAL_VARIABLES].median())\n    return df\n\ntrain = preprocess_data(train)\ntest = preprocess_data(test)\n\ndef features_engineering(df):\n    df['year_hct'] = df['year_hct'] - 2000\n    df['donor_age_diff'] = df['donor_age'] - df['age_at_hct']  # New feature: donor-patient age difference\n    df['hla_mismatch_sum'] = df[HLA_COLUMNS].sum(axis=1)  # New feature: total HLA mismatches\n    return df\n\ntrain = features_engineering(train)\ntest = features_engineering(test)\n\ntrain[CATEGORICAL_VARIABLES] = train[CATEGORICAL_VARIABLES].astype('category')\ntest[CATEGORICAL_VARIABLES] = test[CATEGORICAL_VARIABLES].astype('category')\n\nFEATURES = train.drop(columns=['ID', 'efs', 'efs_time']).columns.tolist()"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "from xgboost import XGBRegressor, XGBClassifier\nimport xgboost\nprint(\"Using XGBoost version\",xgboost.__version__)\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier\n\nFOLDS = 5\nkf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\noof_xgb = np.zeros(len(train))\npred_efs = np.zeros(len(test))\n\nfor i, (train_index, test_index) in enumerate(kf.split(train, train[\"efs\"])):\n\n    print(\"#\"*25)\n    print(f\"### Fold {i+1}\")\n    print(\"#\"*25)\n    \n    x_train = train.loc[train_index, FEATURES].copy()\n    y_train = train.loc[train_index, \"efs\"]\n    x_valid = train.loc[test_index, FEATURES].copy()\n    y_valid = train.loc[test_index, \"efs\"]\n    x_test = test[FEATURES].copy()\n\n    model_xgb = XGBClassifier(\n        device=\"cuda\",\n        max_depth=4,  # Increased for capacity\n        colsample_bytree=0.7,  # Simplified\n        subsample=0.8,  # Simplified\n        n_estimators=10_000,  # Reduced for speed\n        learning_rate=0.03,  # Adjusted for balance\n        eval_metric=\"auc\",\n        early_stopping_rounds=50,\n        objective='binary:logistic',\n        scale_pos_weight=1.5379160847615545,\n        min_child_weight=4,\n        enable_categorical=True,\n        gamma=3.0  # Rounded\n    )\n    model_xgb.fit(\n        x_train, y_train,\n        eval_set=[(x_valid, y_valid)],  \n        verbose=100\n    )\n\n    # INFER OOF (Probabilities)\n    oof_xgb[test_index] = model_xgb.predict_proba(x_valid)[:, 1]\n    # INFER TEST (Probabilities)\n    pred_efs += model_xgb.predict_proba(x_test)[:, 1]\n\n# COMPUTE AVERAGE TEST PREDS\npred_efs /= FOLDS\n\n# EVALUATE PERFORMANCE\naccuracy = accuracy_score(train[\"efs\"], (oof_xgb > 0.5).astype(int))\nf1 = f1_score(train[\"efs\"], (oof_xgb > 0.5).astype(int))\nroc_auc = roc_auc_score(train[\"efs\"], oof_xgb)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"ROC AUC Score: {roc_auc:.4f}\")"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "import numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom torch.utils.data import TensorDataset\nfrom warnings import filterwarnings\n\nfilterwarnings('ignore')\n\ndef get_X_cat(df, cat_cols, transformers=None):\n    \"\"\"\n    Apply a specific categorical data transformer or a LabelEncoder if None.\n    \"\"\"\n    if transformers is None:\n        transformers = [LabelEncoder().fit(df[col]) for col in cat_cols]\n    return transformers, np.array(\n        [transformer.transform(df[col]) for col, transformer in zip(cat_cols, transformers)]\n    ).T\n\ndef preprocess_data(train, val):\n    \"\"\"\n    Standardize numerical variables and transform (Label-encode) categoricals.\n    Fill NA values with mean for numerical.\n    Create torch dataloaders to prepare data for training and evaluation.\n    \"\"\"\n    X_cat_train, X_cat_val, numerical, transformers = get_categoricals(train, val)\n    scaler = StandardScaler()\n    imp = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=True)\n    X_num_train = imp.fit_transform(train[numerical])\n    X_num_train = scaler.fit_transform(X_num_train)\n    X_num_val = imp.transform(val[numerical])\n    X_num_val = scaler.transform(X_num_val)\n    dl_train = init_dl(X_cat_train, X_num_train, train, training=True)\n    dl_val = init_dl(X_cat_val, X_num_val, val)\n    return X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers\n\ndef get_categoricals(train, val):\n    \"\"\"\n    Remove constant categorical columns and transform them using LabelEncoder.\n    Return the label-transformers for each categorical column, categorical dataframes and numerical columns.\n    \"\"\"\n    categorical_cols, numerical = get_feature_types(train)\n    remove = []\n    for col in categorical_cols:\n        if train[col].nunique() == 1:\n            remove.append(col)\n        ind = ~val[col].isin(train[col])\n        if ind.any():\n            val.loc[ind, col] = np.nan\n    categorical_cols = [col for col in categorical_cols if col not in remove]\n    transformers, X_cat_train = get_X_cat(train, categorical_cols)\n    _, X_cat_val = get_X_cat(val, categorical_cols, transformers)\n    return X_cat_train, X_cat_val, numerical, transformers\n\ndef init_dl(X_cat, X_num, df, training=False):\n    \"\"\"\n    Initialize data loaders with 4 dimensions : categorical dataframe, numerical dataframe and target values (efs and efs_time).\n    Notice that efs_time is log-transformed.\n    Fix batch size to 2048 and return dataloader for training or validation depending on training value.\n    \"\"\"\n    ds_train = TensorDataset(\n        torch.tensor(X_cat, dtype=torch.long),\n        torch.tensor(X_num, dtype=torch.float32),\n        torch.tensor(df.efs_time.values, dtype=torch.float32).log(),\n        torch.tensor(df.efs.values, dtype=torch.long)\n    )\n    bs = 2048\n    dl_train = torch.utils.data.DataLoader(ds_train, batch_size=bs, pin_memory=True, shuffle=training)\n    return dl_train\n\ndef get_feature_types(train):\n    \"\"\"\n    Utility function to return categorical and numerical column names.\n    \"\"\"\n    categorical_cols = [col for i, col in enumerate(train.columns) if ((train[col].dtype == \"object\") | (2 < train[col].nunique() < 25))]\n    RMV = [\"ID\", \"efs\", \"efs_time\", \"y\"]\n    FEATURES = [c for c in train.columns if not c in RMV]\n    numerical = [i for i in FEATURES if i not in categorical_cols]\n    return categorical_cols, numerical\n\ndef add_features(df):\n    \"\"\"\n    Create some new features to help the model focus on specific patterns.\n    \"\"\"\n    df['is_cyto_score_same'] = (df['cyto_score'] == df['cyto_score_detail']).astype(int)\n    df['year_hct'] -= 2000\n    df['donor_age_diff'] = df['donor_age'] - df['age_at_hct']  # New feature\n    return df\n\ndef load_data():\n    \"\"\"\n    Load data and add features.\n    \"\"\"\n    test = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n    test = add_features(test)\n    print(\"Test shape:\", test.shape)\n    train = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\n    train = add_features(train)\n    print(\"Train shape:\", train.shape)\n    return test, train"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "import functools\nfrom typing import List\n\nimport pytorch_lightning as pl\nimport numpy as np\nimport torch\nfrom lifelines.utils import concordance_index\nfrom pytorch_lightning.cli import ReduceLROnPlateau\nfrom pytorch_tabular.models.common.layers import ODST\nfrom torch import nn\nfrom pytorch_lightning.utilities import grad_norm\n\n\nclass CatEmbeddings(nn.Module):\n    \"\"\"\n    Embedding module for the categorical dataframe.\n    \"\"\"\n    def __init__(\n        self,\n        projection_dim: int,\n        categorical_cardinality: List[int],\n        embedding_dim: int\n    ):\n        \"\"\"\n        projection_dim: The dimension of the final output after projecting the concatenated embeddings into a lower-dimensional space.\n        categorical_cardinality: A list where each element represents the number of unique categories (cardinality) in each categorical feature.\n        embedding_dim: The size of the embedding space for each categorical feature.\n        self.embeddings: list of embedding layers for each categorical feature.\n        self.projection: sequential neural network that goes from the embedding to the output projection dimension with GELU activation.\n        \"\"\"\n        super(CatEmbeddings, self).__init__()\n        self.embeddings = nn.ModuleList([\n            nn.Embedding(cardinality, embedding_dim)\n            for cardinality in categorical_cardinality\n        ])\n        self.projection = nn.Sequential(\n            nn.Linear(embedding_dim * len(categorical_cardinality), projection_dim),\n            nn.GELU(),\n            nn.Linear(projection_dim, projection_dim)\n        )\n\n    def forward(self, x_cat):\n        \"\"\"\n        Apply the projection on concatened embeddings that contains all categorical features.\n        \"\"\"\n        x_cat = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embeddings)]\n        x_cat = torch.cat(x_cat, dim=1)\n        return self.projection(x_cat)\n\n\nclass NN(nn.Module):\n    \"\"\"\n    Train a model on both categorical embeddings and numerical data.\n    \"\"\"\n    def __init__(\n            self,\n            continuous_dim: int,\n            categorical_cardinality: List[int],\n            embedding_dim: int,\n            projection_dim: int,\n            hidden_dim: int,\n            dropout: float = 0\n    ):\n        \"\"\"\n        continuous_dim: The number of continuous features.\n        categorical_cardinality: A list of integers representing the number of unique categories in each categorical feature.\n        embedding_dim: The dimensionality of the embedding space for each categorical feature.\n        projection_dim: The size of the projected output space for the categorical embeddings.\n        hidden_dim: The number of neurons in the hidden layer of the MLP.\n        dropout: The dropout rate applied in the network.\n        self.embeddings: previous embeddings for categorical data.\n        self.mlp: defines an MLP model with an ODST layer followed by batch normalization and dropout.\n        self.out: linear output layer that maps the output of the MLP to a single value\n        self.dropout: defines dropout\n        Weights initialization with xavier normal algorithm and biases with zeros.\n        \"\"\"\n        super(NN, self).__init__()\n        self.embeddings = CatEmbeddings(projection_dim, categorical_cardinality, embedding_dim)\n        self.mlp = nn.Sequential(\n            ODST(projection_dim + continuous_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.Dropout(dropout)\n        )\n        self.out = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(dropout)\n\n        # initialize weights\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x_cat, x_cont):\n        \"\"\"\n        Create embedding layers for categorical data, concatenate with continous variables.\n        Add dropout and goes through MLP and return raw output and 1-dimensional output as well.\n        \"\"\"\n        x = self.embeddings(x_cat)\n        x = torch.cat([x, x_cont], dim=1)\n        x = self.dropout(x)\n        x = self.mlp(x)\n        return self.out(x), x\n\n\n@functools.lru_cache\ndef combinations(N):\n    \"\"\"\n    calculates all possible 2-combinations (pairs) of a tensor of indices from 0 to N-1, \n    and caches the result using functools.lru_cache for optimization\n    \"\"\"\n    ind = torch.arange(N)\n    comb = torch.combinations(ind, r=2)\n    return comb.cuda()\n\n\nclass LitNN(pl.LightningModule):\n    \"\"\"\n    Main Model creation and losses definition to fully train the model.\n    \"\"\"\n    def __init__(\n            self,\n            continuous_dim: int,\n            categorical_cardinality: List[int],\n            embedding_dim: int,\n            projection_dim: int,\n            hidden_dim: int,\n            lr: float = 1e-3,\n            dropout: float = 0.2,\n            weight_decay: float = 1e-3,\n            aux_weight: float = 0.1,\n            margin: float = 0.5,\n            race_index: int = 0\n    ):\n        \"\"\"\n        continuous_dim: The number of continuous input features.\n        categorical_cardinality: A list of integers, where each element corresponds to the number of unique categories for each categorical feature.\n        embedding_dim: The dimension of the embeddings for the categorical features.\n        projection_dim: The dimension of the projected space after embedding concatenation.\n        hidden_dim: The size of the hidden layers in the feedforward network (MLP).\n        lr: The learning rate for the optimizer.\n        dropout: Dropout probability to avoid overfitting.\n        weight_decay: The L2 regularization term for the optimizer.\n        aux_weight: Weight used for auxiliary tasks.\n        margin: Margin used in some loss functions.\n        race_index: An index that refer to race_group in the input data.\n        \"\"\"\n        super(LitNN, self).__init__()\n        self.save_hyperparameters()\n\n        # Creates an instance of the NN model defined above\n        self.model = NN(\n            continuous_dim=self.hparams.continuous_dim,\n            categorical_cardinality=self.hparams.categorical_cardinality,\n            embedding_dim=self.hparams.embedding_dim,\n            projection_dim=self.hparams.projection_dim,\n            hidden_dim=self.hparams.hidden_dim,\n            dropout=self.hparams.dropout\n        )\n        self.targets = []\n\n        # Defines a small feedforward neural network that performs an auxiliary task with 1-dimensional output\n        self.aux_cls = nn.Sequential(\n            nn.Linear(self.hparams.hidden_dim, self.hparams.hidden_dim // 3),\n            nn.GELU(),\n            nn.Linear(self.hparams.hidden_dim // 3, 1)\n        )\n\n    def on_before_optimizer_step(self, optimizer):\n        \"\"\"\n        Compute the 2-norm for each layer\n        If using mixed precision, the gradients are already unscaled here\n        \"\"\"\n        norms = grad_norm(self.model, norm_type=2)\n        self.log_dict(norms)\n\n    def forward(self, x_cat, x_cont):\n        \"\"\"\n        Forward pass that outputs the 1-dimensional prediction and the embeddings (raw output)\n        \"\"\"\n        x, emb = self.model(x_cat, x_cont)\n        return x.squeeze(1), emb\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"\n        defines how the model processes each batch of data during training.\n        A batch is a combination of : categorical data, continuous data, efs_time (y) and efs event.\n        y_hat is the efs_time prediction on all data and aux_pred is auxiliary prediction on embeddings.\n        Calculates loss and race_group loss on full data.\n        Auxiliary loss is calculated with an event mask, ignoring efs=0 predictions and taking the average.\n        Returns loss and aux_loss multiplied by weight defined above.\n        \"\"\"\n        x_cat, x_cont, y, efs = batch\n        y_hat, emb = self(x_cat, x_cont)\n        aux_pred = self.aux_cls(emb).squeeze(1)\n        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n        aux_loss = nn.functional.mse_loss(aux_pred, y, reduction='none')\n        aux_mask = efs == 1\n        aux_loss = (aux_loss * aux_mask).sum() / aux_mask.sum()\n        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"race_loss\", race_loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n        self.log(\"aux_loss\", aux_loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n        return loss + aux_loss * self.hparams.aux_weight\n\n    def get_full_loss(self, efs, x_cat, y, y_hat):\n        \"\"\"\n        Output loss and race_group loss.\n        \"\"\"\n        loss = self.calc_loss(y, y_hat, efs)\n        race_loss = self.get_race_losses(efs, x_cat, y, y_hat)\n        loss += 0.1 * race_loss\n        return loss, race_loss\n\n    def get_race_losses(self, efs, x_cat, y, y_hat):\n        \"\"\"\n        Calculate loss for each race_group based on deviation/variance.\n        \"\"\"\n        races = torch.unique(x_cat[:, self.hparams.race_index])\n        race_losses = []\n        for race in races:\n            ind = x_cat[:, self.hparams.race_index] == race\n            race_losses.append(self.calc_loss(y[ind], y_hat[ind], efs[ind]))\n        race_loss = sum(race_losses) / len(race_losses)\n        races_loss_std = sum((r - race_loss)**2 for r in race_losses) / len(race_losses)\n        return torch.sqrt(races_loss_std)\n\n    def calc_loss(self, y, y_hat, efs):\n        \"\"\"\n        Most important part of the model : loss function used for training.\n        We face survival data with event indicators along with time-to-event.\n\n        This function computes the main loss by the following the steps :\n        * create all data pairs with \"combinations\" function (= all \"two subjects\" combinations)\n        * make sure that we have at least 1 event in each pair\n        * convert y to +1 or -1 depending on the correct ranking\n        * loss is computed using a margin-based hinge loss\n        * mask is applied to ensure only valid pairs are being used (censored data can't be ranked with event in some cases)\n        * average loss on all pairs is returned\n        \"\"\"\n        N = y.shape[0]\n        comb = combinations(N)\n        comb = comb[(efs[comb[:, 0]] == 1) | (efs[comb[:, 1]] == 1)]\n        pred_left = y_hat[comb[:, 0]]\n        pred_right = y_hat[comb[:, 1]]\n        y_left = y[comb[:, 0]]\n        y_right = y[comb[:, 1]]\n        y = 2 * (y_left > y_right).int() - 1\n        loss = nn.functional.relu(-y * (pred_left - pred_right) + self.hparams.margin)\n        mask = self.get_mask(comb, efs, y_left, y_right)\n        loss = (loss.double() * (mask.double())).sum() / mask.sum()\n        return loss\n\n    def get_mask(self, comb, efs, y_left, y_right):\n        \"\"\"\n        Defines all invalid comparisons :\n        * Case 1: \"Left outlived Right\" but Right is censored\n        * Case 2: \"Right outlived Left\" but Left is censored\n        Masks for case 1 and case 2 are combined using |= operator and inverted using ~ to create a \"valid pair mask\"\n        \"\"\"\n        left_outlived = y_left >= y_right\n        left_1_right_0 = (efs[comb[:, 0]] == 1) & (efs[comb[:, 1]] == 0)\n        mask2 = (left_outlived & left_1_right_0)\n        right_outlived = y_right >= y_left\n        right_1_left_0 = (efs[comb[:, 1]] == 1) & (efs[comb[:, 0]] == 0)\n        mask2 |= (right_outlived & right_1_left_0)\n        mask2 = ~mask2\n        mask = mask2\n        return mask\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"\n        This method defines how the model processes each batch during validation\n        \"\"\"\n        x_cat, x_cont, y, efs = batch\n        y_hat, emb = self(x_cat, x_cont)\n        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n        self.targets.append([y, y_hat.detach(), efs, x_cat[:, self.hparams.race_index]])\n        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n\n    def on_validation_epoch_end(self):\n        \"\"\"\n        At the end of the validation epoch, it computes and logs the concordance index\n        \"\"\"\n        cindex, metric = self._calc_cindex()\n        self.log(\"cindex\", metric, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"cindex_simple\", cindex, on_epoch=True, prog_bar=True, logger=True)\n        self.targets.clear()\n\n    def _calc_cindex(self):\n        \"\"\"\n        Calculate c-index accounting for each race_group or global.\n        \"\"\"\n        y = torch.cat([t[0] for t in self.targets]).cpu().numpy()\n        y_hat = torch.cat([t[1] for t in self.targets]).cpu().numpy()\n        efs = torch.cat([t[2] for t in self.targets]).cpu().numpy()\n        races = torch.cat([t[3] for t in self.targets]).cpu().numpy()\n        metric = self._metric(efs, races, y, y_hat)\n        cindex = concordance_index(y, y_hat, efs)\n        return cindex, metric\n\n    def _metric(self, efs, races, y, y_hat):\n        \"\"\"\n        Calculate c-index accounting for each race_group\n        \"\"\"\n        metric_list = []\n        for race in np.unique(races):\n            y_ = y[races == race]\n            y_hat_ = y_hat[races == race]\n            efs_ = efs[races == race]\n            metric_list.append(concordance_index(y_, y_hat_, efs_))\n        metric = float(np.mean(metric_list) - np.sqrt(np.var(metric_list)))\n        return metric\n\n    def test_step(self, batch, batch_idx):\n        \"\"\"\n        Same as training step but to log test data\n        \"\"\"\n        x_cat, x_cont, y, efs = batch\n        y_hat, emb = self(x_cat, x_cont)\n        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n        self.targets.append([y, y_hat.detach(), efs, x_cat[:, self.hparams.race_index]])\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def on_test_epoch_end(self) -> None:\n        \"\"\"\n        At the end of the test epoch, calculates and logs the concordance index for the test set\n        \"\"\"\n        cindex, metric = self._calc_cindex()\n        self.log(\"test_cindex\", metric, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"test_cindex_simple\", cindex, on_epoch=True, prog_bar=True, logger=True)\n        self.targets.clear()\n\n\n    def configure_optimizers(self):\n        \"\"\"\n        configures the optimizer and learning rate scheduler:\n        * Optimizer: Adam optimizer with weight decay (L2 regularization).\n        * Scheduler: Cosine Annealing scheduler, which adjusts the learning rate according to a cosine curve.\n        \"\"\"\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n        scheduler_config = {\n            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingLR(\n                optimizer,\n                T_max=45,\n                eta_min=6e-3\n            ),\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n            \"strict\": False,\n        }\n\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_config}"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "import json\nimport pytorch_lightning as pl\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nfrom pytorch_lightning.callbacks import LearningRateMonitor, TQDMProgressBar\nfrom pytorch_lightning.callbacks import StochasticWeightAveraging\nfrom sklearn.model_selection import StratifiedKFold\n\npl.seed_everything(42)\n\ndef main(hparams):\n    \"\"\"\n    Main function to train the model.\n    \"\"\"\n    test, train_original = load_data()\n    test['efs_time'] = 1\n    test['efs'] = 1\n    oof_nn_pairwise = np.zeros(len(train_original))\n    test_pred = np.zeros(test.shape[0])\n    categorical_cols, numerical = get_feature_types(train_original)\n    kf = StratifiedKFold(n_splits=5, shuffle=True)\n    for i, (train_index, test_index) in enumerate(\n        kf.split(\n            train_original, train_original.race_group.astype(str) + (train_original.age_at_hct == 0.044).astype(str)\n        )\n    ):\n        tt = train_original.copy()\n        train = tt.iloc[train_index]\n        val = tt.iloc[test_index]\n        X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers = preprocess_data(train, val)\n        model = train_final(X_num_train, dl_train, dl_val, transformers, categorical_cols=categorical_cols)\n        oof_pred, _ = model.cuda().eval()(\n            torch.tensor(X_cat_val, dtype=torch.long).cuda(),\n            torch.tensor(X_num_val, dtype=torch.float32).cuda()\n        )\n        oof_nn_pairwise[test_index] = oof_pred.detach().cpu().numpy()\n        train = tt.iloc[train_index]\n        X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers = preprocess_data(train, test)\n        pred, _ = model.cuda().eval()(\n            torch.tensor(X_cat_val, dtype=torch.long).cuda(),\n            torch.tensor(X_num_val, dtype=torch.float32).cuda()\n        )\n        test_pred += pred.detach().cpu().numpy()\n    return -test_pred, -oof_nn_pairwise\n\ndef train_final(X_num_train, dl_train, dl_val, transformers, hparams=None, categorical_cols=None):\n    \"\"\"\n    Defines model hyperparameters and fit the model.\n    \"\"\"\n    if hparams is None:\n        hparams = {\n            \"embedding_dim\": 16,\n            \"projection_dim\": 112,\n            \"hidden_dim\": 56,\n            \"lr\": 0.06464861983337984,\n            \"dropout\": 0.05463240181423116,\n            \"aux_weight\": 0.26545778308743806,\n            \"margin\": 0.35,  # Adjusted from 0.2588\n            \"weight_decay\": 0.0002773544957610778\n        }\n    model = LitNN(\n        continuous_dim=X_num_train.shape[1],\n        categorical_cardinality=[len(t.classes_) for t in transformers],\n        race_index=categorical_cols.index(\"race_group\"),\n        **hparams\n    )\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_top_k=1)\n    trainer = pl.Trainer(\n        accelerator='cuda',\n        max_epochs=50,  # Reduced from 60\n        log_every_n_steps=6,\n        callbacks=[\n            checkpoint_callback,\n            LearningRateMonitor(logging_interval='epoch'),\n            TQDMProgressBar(),\n            StochasticWeightAveraging(swa_lrs=1e-5, swa_epoch_start=40, annealing_epochs=10)  # Adjusted for epochs\n        ],\n    )\n    trainer.fit(model, dl_train)\n    trainer.test(model, dl_val)\n    return model.eval()"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "hparams = {\n    \"embedding_dim\": 16,\n    \"projection_dim\": 112,\n    \"hidden_dim\": 56,\n    \"lr\": 0.06464861983337984,\n    \"dropout\": 0.05463240181423116,\n    \"aux_weight\": 0.26545778308743806,\n    \"margin\": 0.35,\n    \"weight_decay\": 0.0002773544957610778\n}\npairwise_ranking_pred, pairwise_ranking_oof = main(hparams)\n\ny_true = train[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\ny_pred = train[[\"ID\"]].copy()\ny_pred[\"prediction\"] = pairwise_ranking_oof\nm = score(y_true.copy(), y_pred.copy(), \"ID\")\nprint(f\"\\nPairwise ranking NN CV =\",m)\n\n# Update predictions with classifier mask\nshifts = [0.15, 0.25, 0.35]\nbest_shift = 0.25\nbest_score = -1\nfor shift in shifts:\n    temp_oof = pairwise_ranking_oof.copy()\n    temp_oof[oof_xgb > 0.5] += shift\n    y_pred[\"prediction\"] = temp_oof\n    temp_score = score(y_true.copy(), y_pred.copy(), \"ID\")\n    if temp_score > best_score:\n        best_score = temp_score\n        best_shift = shift\n\npairwise_ranking_oof[oof_xgb > 0.5] += best_shift\ny_pred[\"prediction\"] = pairwise_ranking_oof\nm = score(y_true.copy(), y_pred.copy(), \"ID\")\nprint(f\"\\nPairwise ranking NN with classifier mask -> CV =\",m)\n\npairwise_ranking_pred[pred_efs > 0.5] += best_shift"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "subm_data = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\")\nsubm_data['prediction'] = pairwise_ranking_pred\nsubm_data.to_csv('submission2.csv', index=False)"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "!pip install -q --requirement /kaggle/input/yunbase/Yunbase/requirements.txt  \\\n--no-index --find-links file:/kaggle/input/yunbase/\n\n!pip install -q /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n!pip install -q /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n!pip install -q /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n!pip install -q /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n!pip install -q /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "source_file_path = '/kaggle/input/yunbase/Yunbase/baseline.py'\ntarget_file_path = '/kaggle/working/baseline.py'\nwith open(source_file_path, 'r', encoding='utf-8') as file:\n    content = file.read()\nwith open(target_file_path, 'w', encoding='utf-8') as file:\n    file.write(content)"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "from baseline import Yunbase\nimport pandas as pd#read csv,parquet\nimport numpy as np#for scientific computation of matrices\nfrom  lightgbm import LGBMRegressor,LGBMClassifier,log_evaluation,early_stopping\nfrom catboost import CatBoostRegressor,CatBoostClassifier\nfrom lifelines import KaplanMeierFitter\nimport warnings#avoid some negligible errors\nwarnings.filterwarnings('ignore')\nimport random#provide some function to generate random_seed.\ndef seed_everything(seed):\n    np.random.seed(seed)#numpy's random seed\n    random.seed(seed)#python built-in random seed\nseed_everything(seed=2025)\n\ntrain=pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\ntrain_solution=train[['ID','efs','efs_time','race_group']].copy()\n\ndef logit(p):\n    return np.log(p) - np.log(1 - p)\nmax_efs_time,min_efs_time=80,-100\ntrain['efs_time']=train['efs_time']/(max_efs_time-min_efs_time)\ntrain['efs_time']=train['efs_time'].apply(lambda x:logit(x))\ntrain['efs_time']+=10\nprint(train['efs_time'].max(),train['efs_time'].min())\n\nrace2weight={'American Indian or Alaska Native':0.68,\n'Asian':0.7,'Black or African-American':0.67,\n'More than one race':0.68,\n'Native Hawaiian or other Pacific Islander':0.66,\n'White':0.64}\ntrain['donor_age_diff'] = train['donor_age'] - train['age_at_hct']  # New feature\ntrain['weight']=0.5*train['efs']+0.5\ntrain['raceweight']=train['race_group'].apply(lambda x:race2weight.get(x,1))\ntrain['weight']=train['weight']/train['raceweight']\ntrain.drop(['raceweight'],axis=1,inplace=True)\n\ntrain.head()"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\ndef transform_survival_probability(df, time_col='efs_time', event_col='efs'):\n    kmf = KaplanMeierFitter()\n    kmf.fit(df[time_col], event_observed=df[event_col])\n    survival_probabilities = kmf.survival_function_at_times(df[time_col]).values.flatten()\n    return survival_probabilities\n\nrace_group=sorted(train['race_group'].unique())\nfor race in race_group:\n    train.loc[train['race_group']==race,\"target\"] = transform_survival_probability(train[train['race_group']==race], time_col='efs_time', event_col='efs')\n    gap=0.6*(train.loc[(train['race_group']==race)&(train['efs']==0)]['target'].max()-train.loc[(train['race_group']==race)&(train['efs']==1)]['target'].min())/2\n    train.loc[(train['race_group']==race)&(train['efs']==0),'target']-=gap\n\nsns.histplot(data=train, x='target', hue='efs', element='step', stat='density', common_norm=False)\nplt.legend(title='efs')\nplt.title('Distribution of Target by EFS')\nplt.xlabel('Target')\nplt.ylabel('Density')\nplt.show()\n\ntrain.drop(['efs','efs_time'],axis=1,inplace=True)"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "#nunique=2\nnunique2=[col for col in train.columns if train[col].nunique()==2 and col!='efs']\n#nunique<50\nnunique50=[col for col in train.columns if train[col].nunique()<50 and col not in ['efs','weight']]+['age_group','dri_score_NA']\n\ndef FE(df):\n    print(\"< deal with outlier >\")\n    df['nan_value_each_row'] = df.isnull().sum(axis=1)\n    #year_hct=2020 only 4 rows.\n    df['year_hct']=df['year_hct'].replace(2020,2019)\n    df['age_group']=df['age_at_hct']//10\n    #karnofsky_score 40 only 10 rows.\n    df['karnofsky_score']=df['karnofsky_score'].replace(40,50)\n    #hla_high_res_8=2 only 2 rows.\n    df['hla_high_res_8']=df['hla_high_res_8'].replace(2,3)\n    #hla_high_res_6=0 only 1 row.\n    df['hla_high_res_6']=df['hla_high_res_6'].replace(0,2)\n    #hla_high_res_10=3 only 1 row.\n    df['hla_high_res_10']=df['hla_high_res_10'].replace(3,4)\n    #hla_low_res_8=2 only 1 row.\n    df['hla_low_res_8']=df['hla_low_res_8'].replace(2,3)\n    df['dri_score']=df['dri_score'].replace('Missing disease status','N/A - disease not classifiable')\n    df['dri_score_NA']=df['dri_score'].apply(lambda x:int('N/A' in str(x)))\n    for col in ['diabetes','pulm_moderate','cardiac']:\n        df.loc[df[col].isna(),col]='Not done'\n\n    print(\"< cross feature >\")\n    df['donor_age-age_at_hct']=df['donor_age']-df['age_at_hct']\n    df['comorbidity_score+karnofsky_score']=df['comorbidity_score']+df['karnofsky_score']\n    df['comorbidity_score-karnofsky_score']=df['comorbidity_score']-df['karnofsky_score']\n    df['comorbidity_score*karnofsky_score']=df['comorbidity_score']*df['karnofsky_score']\n    df['comorbidity_score/karnofsky_score']=df['comorbidity_score']/df['karnofsky_score']\n    \n    print(\"< fillna >\")\n    df[nunique50]=df[nunique50].astype(str).fillna('NaN')\n    \n    print(\"< combine category feature >\")\n    for i in range(len(nunique2)):\n        for j in range(i+1,len(nunique2)):\n            df[nunique2[i]+nunique2[j]]=df[nunique2[i]].astype(str)+df[nunique2[j]].astype(str)\n    \n    print(\"< drop useless columns >\")\n    df.drop(['ID'],axis=1,inplace=True,errors='ignore')\n    return df\n\ncombine_category_cols=[]\nfor i in range(len(nunique2)):\n    for j in range(i+1,len(nunique2)):\n        combine_category_cols.append(nunique2[i]+nunique2[j])  \n\ntotal_category_feature=nunique50+combine_category_cols\n\ntarget_stat=[]\nfor j in range(len(total_category_feature)):\n   for col in ['donor_age','age_at_hct','target']:\n    target_stat.append( (total_category_feature[j],col,['count','mean','max','std','skew']) )\n\nnum_folds=5  # Reduced from 10\n\nlgb_params={\"boosting_type\": \"gbdt\",\"metric\": 'mae',\n            'random_state': 2025,  \"max_depth\": 9,\"learning_rate\": 0.1,\n            \"n_estimators\": 500,  # Reduced from 768\n            \"colsample_bytree\": 0.6,\"colsample_bynode\": 0.6,\n            \"verbose\": -1,\"reg_alpha\": 0.2,\n            \"reg_lambda\": 5,\"extra_trees\":True,'num_leaves':64,\"max_bin\":255,\n            'importance_type': 'gain',\n            'device':'gpu','gpu_use_dp':True\n           }\n\ncat_params={'random_state':2025,'eval_metric' : 'MAE',\n            'bagging_temperature': 0.50,'iterations': 500,  # Reduced from 650\n            'learning_rate': 0.1,'max_depth': 8,\n            'l2_leaf_reg': 1.25,'min_data_in_leaf': 24,\n            'random_strength' : 0.25, 'verbose': 0,\n            'task_type':'GPU',\n            }\n\nyunbase=Yunbase(num_folds=num_folds,\n                  models=[(LGBMRegressor(**lgb_params),'lgb'),\n                          (CatBoostRegressor(**cat_params),'cat'),\n                         ],\n                  FE=FE,\n                  seed=2025,\n                  objective='regression',\n                  metric='mae',\n                  target_col='target',\n                  device='gpu',\n                  one_hot_max=-1,\n                  early_stop=1000,\n                  cross_cols=['donor_age','age_at_hct'],\n                  target_stat=target_stat,\n                  use_data_augmentation=True,\n                  use_scaler=True,\n                  log=250,\n                  plot_feature_importance=True,\n                  use_eval_metric=False,\n)\nyunbase.fit(train,category_cols=nunique2)"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "import pandas.api.types\nfrom lifelines.utils import concordance_index\n\nclass ParticipantVisibleError(Exception):\n    pass\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \n    del solution[row_id_column_name]\n    del submission[row_id_column_name]\n    \n    event_label = 'efs'\n    interval_label = 'efs_time'\n    prediction_label = 'prediction'\n    for col in submission.columns:\n        if not pandas.api.types.is_numeric_dtype(submission[col]):\n            raise ParticipantVisibleError(f'Submission column {col} must be a number')\n    # Merging solution and submission dfs on ID\n    merged_df = pd.concat([solution, submission], axis=1)\n    merged_df.reset_index(inplace=True)\n    merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n    metric_list = []\n    for race in merged_df_race_dict.keys():\n        # Retrieving values from y_test based on index\n        indices = sorted(merged_df_race_dict[race])\n        merged_df_race = merged_df.iloc[indices]\n        # Calculate the concordance index\n        c_index_race = concordance_index(\n                        merged_df_race[interval_label],\n                        -merged_df_race[prediction_label],\n                        merged_df_race[event_label])\n        metric_list.append(c_index_race)\n    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))\n\nweights = [0.55, 0.45]  # Slight preference to LightGBM\n\nlgb_prediction=np.load(f\"Yunbase_info/lgb_seed{yunbase.seed}_repeat0_fold{yunbase.num_folds}_{yunbase.target_col}.npy\")\nlgb_prediction=pd.DataFrame({'ID':train_solution['ID'],'prediction':lgb_prediction})\nprint(f\"lgb_score:{score(train_solution.copy(),lgb_prediction.copy(),row_id_column_name='ID')}\")\n\ncat_prediction=np.load(f\"Yunbase_info/cat_seed{yunbase.seed}_repeat0_fold{yunbase.num_folds}_{yunbase.target_col}.npy\")\ncat_prediction=pd.DataFrame({'ID':train_solution['ID'],'prediction':cat_prediction})\nprint(f\"cat_score:{score(train_solution.copy(),cat_prediction.copy(),row_id_column_name='ID')}\")\n\ny_preds=[\n    lgb_prediction.copy(),\n    cat_prediction.copy()\n]\n\nfinal_prediction=lgb_prediction.copy()\nfinal_prediction['prediction']=0\nfor i in range(len(y_preds)):\n    final_prediction['prediction']+=weights[i]*y_preds[i]['prediction']\nmetric=score(train_solution.copy(),final_prediction.copy(),row_id_column_name='ID')\nprint(f\"final_CV:{metric}\")"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "# Add donor_age_diff to test data before prediction\ntest['donor_age_diff'] = test['donor_age'] - test['age_at_hct']  # Compute the feature\ntest_preds = yunbase.predict(test, weights=weights)  # Weights [0.55, 0.45] from Block 21\nyunbase.target_col = 'prediction'\nyunbase.submit(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\", test_preds,\n               save_name='submission1'\n              )"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "!pip install /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n!pip install /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "import warnings\nfrom pathlib import Path\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport polars as pl\nimport pandas as pd\nimport plotly.colors as pc\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport plotly.io as pio\npio.renderers.default = 'iframe'\n\npd.options.display.max_columns = None\n\nfrom lifelines import CoxPHFitter\nfrom lifelines import KaplanMeierFitter\nfrom lifelines import NelsonAalenFitter\nimport lightgbm as lgb\nfrom metric import score\nfrom scipy.stats import rankdata \nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import KFold"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "class CFG:\n\n    train_path = Path('/kaggle/input/equity-post-HCT-survival-predictions/train.csv')\n    test_path = Path('/kaggle/input/equity-post-HCT-survival-predictions/test.csv')\n    subm_path = Path('/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv')\n    \n    colorscale = 'Redor'\n    color = '#A2574F'\n\n    batch_size = 32768\n    early_stop = 300\n    penalizer = 0.01\n    n_splits = 5\n\n    weights = [3, 2, 4, 2, 6, 3, 6, 6]  # Adjusted to favor Cox (target1) and target4\n\n    ctb_params = {\n        'loss_function': 'RMSE',\n        'learning_rate': 0.03,\n        'random_state': 42,\n        'task_type': 'CPU',\n        'num_trees': 4000,  # Reduced from 6000\n        'reg_lambda': 8.0,\n        'depth': 8\n    }\n\n    lgb_params = {\n        'objective': 'regression',\n        'min_child_samples': 32,\n        'num_iterations': 4000,  # Reduced from 6000\n        'learning_rate': 0.03,\n        'extra_trees': True,\n        'reg_lambda': 8.0,\n        'reg_alpha': 0.1,\n        'num_leaves': 64,\n        'metric': 'rmse',\n        'max_depth': 8,\n        'device': 'cpu',\n        'max_bin': 128,\n        'verbose': -1,\n        'seed': 42\n    }\n\n    cox1_params = {\n        'grow_policy': 'Depthwise',\n        'min_child_samples': 8,\n        'loss_function': 'Cox',\n        'learning_rate': 0.03,\n        'random_state': 42,\n        'task_type': 'CPU',\n        'num_trees': 4000,  # Reduced from 6000\n        'reg_lambda': 8.0,\n        'depth': 8\n    }\n\n    cox2_params = {\n        'grow_policy': 'Lossguide',\n        'min_child_samples': 2,\n        'loss_function': 'Cox',\n        'learning_rate': 0.03,\n        'random_state': 42,\n        'task_type': 'CPU',\n        'num_trees': 4000,  # Reduced from 6000\n        'reg_lambda': 8.0,\n        'num_leaves': 32,\n        'depth': 8\n    }"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "class FE:\n\n    def __init__(self, batch_size):\n        self._batch_size = batch_size\n\n    def _load_data(self, path):\n        return pl.read_csv(path, batch_size=self._batch_size)\n\n    def _update_hla_columns(self, df):\n        \n        df = df.with_columns(\n            \n            pl.col('hla_match_a_low').fill_null(0)\n            .add(pl.col('hla_match_b_low').fill_null(0))\n            .add(pl.col('hla_match_drb1_high').fill_null(0))\n            .alias('hla_nmdp_6'),\n            \n            pl.col('hla_match_a_low').fill_null(0)\n            .add(pl.col('hla_match_b_low').fill_null(0))\n            .add(pl.col('hla_match_drb1_low').fill_null(0))\n            .alias('hla_low_res_6'),\n            \n            pl.col('hla_match_a_high').fill_null(0)\n            .add(pl.col('hla_match_b_high').fill_null(0))\n            .add(pl.col('hla_match_drb1_high').fill_null(0))\n            .alias('hla_high_res_6'),\n            \n            pl.col('hla_match_a_low').fill_null(0)\n            .add(pl.col('hla_match_b_low').fill_null(0))\n            .add(pl.col('hla_match_c_low').fill_null(0))\n            .add(pl.col('hla_match_drb1_low').fill_null(0))\n            .alias('hla_low_res_8'),\n            \n            pl.col('hla_match_a_high').fill_null(0)\n            .add(pl.col('hla_match_b_high').fill_null(0))\n            .add(pl.col('hla_match_c_high').fill_null(0))\n            .add(pl.col('hla_match_drb1_high').fill_null(0))\n            .alias('hla_high_res_8'),\n            \n            pl.col('hla_match_a_low').fill_null(0)\n            .add(pl.col('hla_match_b_low').fill_null(0))\n            .add(pl.col('hla_match_c_low').fill_null(0))\n            .add(pl.col('hla_match_drb1_low').fill_null(0))\n            .add(pl.col('hla_match_dqb1_low').fill_null(0))\n            .alias('hla_low_res_10'),\n            \n            pl.col('hla_match_a_high').fill_null(0)\n            .add(pl.col('hla_match_b_high').fill_null(0))\n            .add(pl.col('hla_match_c_high').fill_null(0))\n            .add(pl.col('hla_match_drb1_high').fill_null(0))\n            .add(pl.col('hla_match_dqb1_high').fill_null(0))\n            .alias('hla_high_res_10'),\n            \n            (pl.col('donor_age').fill_null(-1) - pl.col('age_at_hct').fill_null(-1)).alias('donor_age_diff')  # New feature\n        )\n    \n        return df\n\n    def _cast_datatypes(self, df):\n\n        num_cols = [\n            'hla_high_res_8',\n            'hla_low_res_8',\n            'hla_high_res_6',\n            'hla_low_res_6',\n            'hla_high_res_10',\n            'hla_low_res_10',\n            'hla_match_dqb1_high',\n            'hla_match_dqb1_low',\n            'hla_match_drb1_high',\n            'hla_match_drb1_low',\n            'hla_nmdp_6',\n            'year_hct',\n            'hla_match_a_high',\n            'hla_match_a_low',\n            'hla_match_b_high',\n            'hla_match_b_low',\n            'hla_match_c_high',\n            'hla_match_c_low',\n            'donor_age',\n            'age_at_hct',\n            'comorbidity_score',\n            'karnofsky_score',\n            'efs',\n            'efs_time',\n            'donor_age_diff'  # Added to num_cols\n        ]\n\n        for col in df.columns:\n\n            if col in num_cols:\n                df = df.with_columns(pl.col(col).fill_null(-1).cast(pl.Float32))  \n\n            else:\n                df = df.with_columns(pl.col(col).fill_null('Unknown').cast(pl.String))  \n\n        return df.with_columns(pl.col('ID').cast(pl.Int32))\n\n    def info(self, df):\n        \n        print(f'\\nShape of dataframe: {df.shape}') \n        \n        mem = df.memory_usage().sum() / 1024**2\n        print('Memory usage: {:.2f} MB\\n'.format(mem))\n\n        display(df.head())\n\n    def apply_fe(self, path):\n\n        df = self._load_data(path)   \n        df = self._update_hla_columns(df)                     \n        df = self._cast_datatypes(df)        \n        df = df.to_pandas()\n        self.info(df)\n        \n        cat_cols = [col for col in df.columns if df[col].dtype == pl.String]\n\n        return df, cat_cols\n\nfe = FE(CFG.batch_size)\ntrain_data, cat_cols = fe.apply_fe(CFG.train_path)"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "test_data, _ = fe.apply_fe(CFG.test_path)"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "class EDA:\n    \n    def __init__(self, colorscale, color, data):\n        self._colorscale = colorscale\n        self._color = color  \n        self.data = data\n\n    def _template(self, fig, title):\n        fig.update_layout(\n            title=title,\n            title_x=0.5, \n            plot_bgcolor='rgba(247, 230, 202, 1)',  \n            paper_bgcolor='rgba(247, 230, 202, 1)', \n            font=dict(color=self._color),\n            margin=dict(l=72, r=72, t=72, b=72), \n            height=720\n        )\n        return fig\n\n    def distribution_plot(self, col, title):\n        fig = px.histogram(\n            self.data,\n            x=col,\n            nbins=100,\n            color_discrete_sequence=[self._color]\n        )\n        fig.update_layout(\n            xaxis_title='Values',\n            yaxis_title='Count',\n            bargap=0.1,\n            xaxis=dict(gridcolor='grey'),\n            yaxis=dict(gridcolor='grey', zerolinecolor='grey')\n        )\n        fig.update_traces(hovertemplate='Value: %{x:.2f}<br>Count: %{y:,}')\n        fig = self._template(fig, f'{title}')\n        fig.show()\n    \n    def bar_chart(self, col):\n        value_counts = self.data[col].value_counts().reset_index()\n        value_counts.columns = [col, 'count']\n        fig = px.bar(\n            value_counts,\n            y=col,\n            x='count',\n            orientation='h',\n            color='count',\n            color_continuous_scale=self._colorscale,\n        )\n        fig.update_layout(\n            xaxis_title='Count',\n            yaxis_title='',\n            xaxis=dict(gridcolor='grey'),\n            yaxis=dict(gridcolor='grey', zerolinecolor='grey')\n        )\n        fig.update_traces(\n            hovertemplate=(\n                f'<b>{col}:</b> %{{y}}<br>'\n                '<b>Count:</b> %{x:,}<br>'\n            ),\n            hoverlabel=dict(\n                font=dict(color=self._color),\n                bgcolor='rgba(247, 230, 202, 1)'\n            )\n        )\n        fig = self._template(fig, f'{col}')\n        fig.show()\n        \n    def _plot_cv(self, scores, title, metric='Stratified C-Index'):\n        fold_scores = [round(score, 3) for score in scores]\n        mean_score = round(np.mean(scores), 3)\n        fig = go.Figure()\n        fig.add_trace(go.Scatter(\n            x = list(range(1, len(fold_scores) + 1)),\n            y = fold_scores,\n            mode = 'markers', \n            name = 'Fold Scores',\n            marker = dict(size = 27, color=self._color, symbol='diamond'),\n            text = [f'{score:.3f}' for score in fold_scores],\n            hovertemplate = 'Fold %{x}: %{text}<extra></extra>',\n            hoverlabel = dict(font=dict(size=18))  \n        ))\n        fig.add_trace(go.Scatter(\n            x = [1, len(fold_scores)],\n            y = [mean_score, mean_score],\n            mode = 'lines',\n            name = f'Mean: {mean_score:.3f}',\n            line = dict(dash = 'dash', color = '#B22222'),\n            hoverinfo = 'none'\n        ))\n        fig.update_layout(\n            title = f'{title} | Cross-validation Mean {metric} Score: {mean_score}',\n            xaxis_title = 'Fold',\n            yaxis_title = f'{metric} Score',\n            plot_bgcolor = 'rgba(247, 230, 202, 1)',  \n            paper_bgcolor = 'rgba(247, 230, 202, 1)',\n            font = dict(color=self._color), \n            xaxis = dict(\n                gridcolor = 'grey',\n                tickmode = 'linear',\n                tick0 = 1,\n                dtick = 1,\n                range = [0.5, len(fold_scores) + 0.5],\n                zerolinecolor = 'grey'\n            ),\n            yaxis = dict(\n                gridcolor = 'grey',\n                zerolinecolor = 'grey'\n            )\n        )\n        fig.show()\n\nclass Targets:\n\n    def __init__(self, data, cat_cols, penalizer, n_splits):\n        \n        self.data = data\n        self.cat_cols = cat_cols\n        \n        self._length = len(self.data)\n        self._penalizer = penalizer\n        self._n_splits = n_splits\n\n    def _prepare_cv(self):\n        \n        oof_preds = np.zeros(self._length)\n            \n        cv = KFold(n_splits=self._n_splits, shuffle=True, random_state=42)\n\n        return cv, oof_preds\n\n    def validate_model(self, preds, title):\n            \n        y_true = self.data[['ID', 'efs', 'efs_time', 'race_group']].copy()\n        y_pred = self.data[['ID']].copy()\n        \n        y_pred['prediction'] = preds\n            \n        c_index_score = score(y_true.copy(), y_pred.copy(), 'ID')\n        print(f'Overall Stratified C-Index Score for {title}: {c_index_score:.4f}')\n\n    def create_target1(self):  \n\n        '''\n        Constant columns are dropped if they exist in a fold. Otherwise, the code produces error:\n\n        delta contains nan value(s). Convergence halted. Please see the following tips in the lifelines documentation: \n        https://lifelines.readthedocs.io/en/latest/Examples.html#problems-with-convergence-in-the-cox-proportional-hazard-model\n        '''\n\n        cv, oof_preds = self._prepare_cv()\n\n        # Apply one hot encoding to categorical columns\n        data = pd.get_dummies(self.data, columns=self.cat_cols, drop_first=True).drop('ID', axis=1)\n\n        for train_index, valid_index in cv.split(data):\n\n            train_data = data.iloc[train_index]\n            valid_data = data.iloc[valid_index]\n\n            # Drop constant columns if they exist\n            train_data = train_data.loc[:, train_data.nunique() > 1]\n            valid_data = valid_data[train_data.columns]\n\n            cph = CoxPHFitter(penalizer=0.1)  # Increased penalizer, no stratification\n            cph.fit(train_data, duration_col='efs_time', event_col='efs')\n            \n            oof_preds[valid_index] = cph.predict_partial_hazard(valid_data)              \n\n        self.data['target1'] = oof_preds \n        self.validate_model(oof_preds, 'Cox') \n\n        return self.data\n\n    def create_target2(self):        \n\n        cv, oof_preds = self._prepare_cv()\n        baseline_kmf = KaplanMeierFitter()\n        baseline_kmf.fit(self.data['efs_time'], self.data['efs'])  # Overall baseline fit\n\n        for train_index, valid_index in cv.split(self.data):\n\n            train_data = self.data.iloc[train_index]\n            valid_data = self.data.iloc[valid_index]\n\n            kmf = KaplanMeierFitter()\n            kmf.fit(durations=train_data['efs_time'], event_observed=train_data['efs'])\n            \n            fold_preds = kmf.survival_function_at_times(valid_data['efs_time']).values\n            baseline_preds = baseline_kmf.survival_function_at_times(valid_data['efs_time']).values\n            oof_preds[valid_index] = 0.7 * fold_preds + 0.3 * baseline_preds  # Blend to reduce overfitting\n\n        self.data['target2'] = oof_preds  \n        self.validate_model(oof_preds, 'Kaplan-Meier')\n\n        return self.data\n\n    def create_target3(self):        \n\n        cv, oof_preds = self._prepare_cv()\n        baseline_naf = NelsonAalenFitter()\n        baseline_naf.fit(self.data['efs_time'], self.data['efs'])  # Overall baseline fit\n\n        for train_index, valid_index in cv.split(self.data):\n\n            train_data = self.data.iloc[train_index]\n            valid_data = self.data.iloc[valid_index]\n            \n            naf = NelsonAalenFitter()\n            naf.fit(durations=train_data['efs_time'], event_observed=train_data['efs'])\n            \n            fold_preds = -naf.cumulative_hazard_at_times(valid_data['efs_time']).values\n            baseline_preds = -baseline_naf.cumulative_hazard_at_times(valid_data['efs_time']).values\n            oof_preds[valid_index] = 0.7 * fold_preds + 0.3 * baseline_preds  # Blend to reduce overfitting\n\n        self.data['target3'] = oof_preds  \n        self.validate_model(oof_preds, 'Nelson-Aalen')\n\n        return self.data\n\n    def create_target4(self):\n\n        self.data['target4'] = self.data.efs_time.copy()\n        self.data.loc[self.data.efs == 0, 'target4'] *= -1\n\n        self.validate_model(self.data['target4'], 'Signed efs_time')  # Log CV score\n        return self.data\n\nclass MD:\n    \n    def __init__(self, colorscale, color, data, cat_cols, early_stop, penalizer, n_splits):\n        \n        self.eda = EDA(colorscale, color, data)\n        self.targets = Targets(data, cat_cols, penalizer, n_splits)\n        \n        self.data = data\n        self.cat_cols = cat_cols\n        self._early_stop = early_stop\n\n    def create_targets(self):\n\n        self.data = self.targets.create_target1()\n        self.data = self.targets.create_target2()\n        self.data = self.targets.create_target3()\n        self.data = self.targets.create_target4()\n\n        return self.data\n        \n    def train_model(self, params, target, title):\n        \n        for col in self.cat_cols:\n            self.data[col] = self.data[col].astype('category')\n            \n        X = self.data.drop(['ID', 'efs', 'efs_time', 'target1', 'target2', 'target3', 'target4'], axis=1)\n        y = self.data[target]\n        \n        models, fold_scores = [], []\n            \n        cv, oof_preds = self.targets._prepare_cv()\n    \n        for fold, (train_index, valid_index) in enumerate(cv.split(X, y)):\n                \n            X_train = X.iloc[train_index]\n            X_valid = X.iloc[valid_index]\n                \n            y_train = y.iloc[train_index]\n            y_valid = y.iloc[valid_index]\n    \n            if title.startswith('LightGBM'):\n                        \n                model = lgb.LGBMRegressor(**params)\n                model.fit(\n                    X_train, \n                    y_train,  \n                    eval_set=[(X_valid, y_valid)],\n                    eval_metric='rmse',\n                    callbacks=[lgb.early_stopping(self._early_stop, verbose=0), lgb.log_evaluation(0)]\n                )\n                        \n            elif title.startswith('CatBoost'):\n                        \n                model = CatBoostRegressor(**params, verbose=0, cat_features=self.cat_cols)\n                model.fit(\n                    X_train,\n                    y_train,\n                    eval_set=(X_valid, y_valid),\n                    early_stopping_rounds=self._early_stop, \n                    verbose=0\n                )               \n                    \n            models.append(model)\n                \n            oof_preds[valid_index] = model.predict(X_valid)\n\n            y_true_fold = self.data.iloc[valid_index][['ID', 'efs', 'efs_time', 'race_group']].copy()\n            y_pred_fold = self.data.iloc[valid_index][['ID']].copy()\n            \n            y_pred_fold['prediction'] = oof_preds[valid_index]\n    \n            fold_score = score(y_true_fold, y_pred_fold, 'ID')\n            fold_scores.append(fold_score)\n    \n        self.eda._plot_cv(fold_scores, title)\n        self.targets.validate_model(oof_preds, title)\n        \n        return models, oof_preds\n\n    def infer_model(self, data, models):\n        \n        data = data.drop(['ID'], axis=1)\n\n        for col in self.cat_cols:\n            data[col] = data[col].astype('category')\n\n        return np.mean([model.predict(data) for model in models], axis=0)"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "md = MD(CFG.colorscale, CFG.color, train_data, cat_cols, CFG.early_stop, CFG.penalizer, CFG.n_splits)"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "train_data = md.create_targets()"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "fe.info(train_data)"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "ctb1_models, ctb1_oof_preds = md.train_model(CFG.ctb_params, target='target1', title='CatBoost')"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "lgb1_models, lgb1_oof_preds = md.train_model(CFG.lgb_params, target='target1', title='LightGBM')"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "ctb1_preds = md.infer_model(test_data, ctb1_models)"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "lgb1_preds = md.infer_model(test_data, lgb1_models)"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "ctb2_models, ctb2_oof_preds = md.train_model(CFG.ctb_params, target='target2', title='CatBoost')"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "lgb2_models, lgb2_oof_preds = md.train_model(CFG.lgb_params, target='target2', title='LightGBM')"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "ctb2_preds = md.infer_model(test_data, ctb2_models)"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "lgb2_preds = md.infer_model(test_data, lgb2_models)"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "ctb3_models, ctb3_oof_preds = md.train_model(CFG.ctb_params, target='target3', title='CatBoost')"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "lgb3_models, lgb3_oof_preds = md.train_model(CFG.lgb_params, target='target3', title='LightGBM')"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "ctb3_preds = md.infer_model(test_data, ctb3_models)"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "lgb3_preds = md.infer_model(test_data, lgb3_models)"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "cox1_models, cox1_oof_preds = md.train_model(CFG.cox1_params, target='target4', title='CatBoost')"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "cox2_models, cox2_oof_preds = md.train_model(CFG.cox2_params, target='target4', title='CatBoost')"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "cox1_preds = md.infer_model(test_data, cox1_models)"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "cox2_preds = md.infer_model(test_data, cox2_models)"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "oof_preds = [\n    ctb1_oof_preds, \n    lgb1_oof_preds, \n    ctb2_oof_preds, \n    lgb2_oof_preds, \n    ctb3_oof_preds, \n    lgb3_oof_preds, \n    cox1_oof_preds,\n    cox2_oof_preds\n]\n\npreds = [\n    ctb1_preds, \n    lgb1_preds, \n    ctb2_preds, \n    lgb2_preds, \n    ctb3_preds, \n    lgb3_preds,\n    cox1_preds,\n    cox2_preds\n]"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "ranked_oof_preds = np.array([rankdata(p) for p in oof_preds])\nensemble_oof_preds = np.dot(CFG.weights, ranked_oof_preds)\nmd.targets.validate_model(ensemble_oof_preds, 'Ensemble Model')"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "ranked_preds = np.array([rankdata(p) for p in preds])\nensemble_preds = np.dot(CFG.weights, ranked_preds)\nsubm_data = pd.read_csv(CFG.subm_path)\nsubm_data['prediction'] = ensemble_preds"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "subm_data.to_csv('submission3.csv', index=False)\ndisplay(subm_data.head())"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import rankdata\nfrom metric import score  # Ensure you have this function\n\n# Load submission files\nsub1 = pd.read_csv('/kaggle/working/submission1.csv')  # Yunbase\nsub2 = pd.read_csv('/kaggle/working/submission2.csv')  # PRL-NN\nsub3 = pd.read_csv('/kaggle/working/submission3.csv')  # EDA & Ensemble\n\n# Ensure all predictions have the same length\nassert len(sub1) == len(sub2) == len(sub3), \"\u274c Error: Submission files have different lengths!\"\n\n# Calculate ranks for test predictions\nrank1 = rankdata(sub1['prediction'], method='average')  # Yunbase\nrank2 = rankdata(sub2['prediction'], method='average')  # PRL-NN\nrank3 = rankdata(sub3['prediction'], method='average')  # EDA\n\n# \ud83d\udd39 Debug: Print submission file lengths\nprint(f\"\u2705 Length of submission1.csv: {len(sub1)}\")\nprint(f\"\u2705 Length of submission2.csv: {len(sub2)}\")\nprint(f\"\u2705 Length of submission3.csv: {len(sub3)}\")\n\n# Load OOF predictions (Ensure they have the correct shape)\noof_preds_list = [\n    np.array(ctb1_oof_preds), np.array(lgb1_oof_preds),  # target1\n    np.array(ctb2_oof_preds), np.array(lgb2_oof_preds),  # target2\n    np.array(ctb3_oof_preds), np.array(lgb3_oof_preds),  # target3\n    np.array(cox1_oof_preds), np.array(cox2_oof_preds)   # target4\n]\n\n# Stack OOF predictions correctly (rows = samples, columns = models)\noof_preds = np.column_stack(oof_preds_list)  # Shape: (n_train, num_models)\n\n# Compute mean across models\nensemble_oof_preds = np.mean(oof_preds, axis=1)  # Shape: (n_train,)\n\n# \ud83d\udd39 Debug: Check shapes\nprint(f\"\ud83d\udd0d Shape of ensemble_oof_preds: {ensemble_oof_preds.shape}\")\nprint(f\"\ud83d\udd0d Shape of sub1['prediction']: {sub1['prediction'].shape}\")\n\n# \ud83d\udea8 Check for shape mismatch\nif len(ensemble_oof_preds) != len(sub1['prediction']):\n    raise ValueError(f\"\u274c Shape Mismatch: ensemble_oof_preds ({len(ensemble_oof_preds)}) vs sub1 ({len(sub1['prediction'])})\")\n\n# Optimize weights using EDA OOF as proxy\ny_true = train_data[['ID', 'efs', 'efs_time', 'race_group']].copy()\ny_pred = train_data[['ID']].copy()\n\nbest_score = -1\nbest_weights = [0.33, 0.34, 0.33]\nfor w1 in [0.30, 0.32, 0.34]:  # Yunbase\n    for w2 in [0.33, 0.35, 0.37]:  # PRL-NN\n        w3 = 1 - w1 - w2  # EDA\n        if w3 < 0.30 or w3 > 0.36:\n            continue\n\n        # Apply ranking ensemble with proper shape\n        y_pred['prediction'] = (\n            w1 * rankdata(sub1['prediction'], method='average') +\n            w2 * rankdata(sub2['prediction'], method='average') +\n            w3 * rankdata(sub3['prediction'], method='average')  # Fixed shape\n        )\n\n        temp_score = score(y_true.copy(), y_pred.copy(), 'ID')\n        if temp_score > best_score:\n            best_score = temp_score\n            best_weights = [w1, w2, w3]\n\nprint(f\"\u2705 Best CV Score: {best_score:.4f} with weights {best_weights}\")\n\n# Apply optimized weights to test ranks\nrank_df = pd.DataFrame({\n    'rank1': rank1,\n    'rank2': rank2,\n    'rank3': rank3\n})\nensemble_rank = (\n    best_weights[0] * rank_df['rank1'] +\n    best_weights[1] * rank_df['rank2'] +\n    best_weights[2] * rank_df['rank3']\n)\n\n# Create final submission\nfinal_sub = sub1[['ID']].copy()\nfinal_sub['prediction'] = ensemble_rank\nfinal_sub.to_csv('submission.csv', index=False)\n\nprint(\"\ud83c\udfaf Final Submission Saved as submission.csv\")\n"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}